{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95503eb",
   "metadata": {},
   "source": [
    "# What Are Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "RNNs are a powerful and robust type of neural network, and belong to the most promising algorithms in use because they are the only type of neural network with an internal memory.\n",
    "\n",
    "Like many other deep learning algorithms, recurrent neural networks are relatively old. They were initially created in the 1980s, but only in recent years have we seen their true potential. An increase in computational power along with the massive amounts of data that we now have to work with, and the invention of long short-term memory (LSTM) in the 1990s, has really brought RNNs to the foreground.\n",
    "\n",
    "Because of their internal memory, RNNs can remember important things about the input they received, which allows them to be very precise in predicting what’s coming next. This is why they’re the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more. Recurrent neural networks can form a much deeper understanding of a sequence and its context compared to other algorithms.\n",
    " \n",
    "# How Do Recurrent Neural Networks Work?\n",
    "\n",
    "To understand RNNs properly, you’ll need a working knowledge of “normal” feed-forward neural networks and sequential data. \n",
    "\n",
    "Sequential data is basically just ordered data in which related things follow each other. Examples are financial data or the DNA sequence. The most popular type of sequential data is perhaps time series data, which is just a series of data points that are listed in time order.\n",
    "\n",
    "![](2024-03-01-22-08-31.png)\n",
    " \n",
    "# Recurrent vs. Feed-Forward Neural Networks\n",
    "\n",
    "**feed forward neural network**\n",
    "\n",
    "RNNs and feed-forward neural networks get their names from the way they channel information.\n",
    "\n",
    "In a feed-forward neural network, the information only moves in one direction — from the input layer, through the hidden layers, to the output layer. The information moves straight through the network.\n",
    "\n",
    "Feed-forward neural networks have no memory of the input they receive and are bad at predicting what’s coming next. Because a feed-forward network only considers the current input, it has no notion of order in time. It simply can’t remember anything about what happened in the past except its training.\n",
    "\n",
    "In an RNN, the information cycles through a loop. When it makes a decision, it considers the current input and also what it has learned from the inputs it received previously.\n",
    "\n",
    "The two images below illustrate the difference in information flow between an RNN and a feed-forward neural network.\n",
    "\n",
    "![](2024-03-01-22-09-19.png)\n",
    "\n",
    "A usual RNN has a short-term memory. In combination with an LSTM they also have a long-term memory (more on that later).\n",
    "\n",
    "Another good way to illustrate the concept of a recurrent neural network’s memory is to explain it with an example: Imagine you have a normal feed-forward neural network and give it the word “neuron” as an input and it processes the word character by character. By the time it reaches the character “r,” it has already forgotten about “n,” “e” and “u,” which makes it almost impossible for this type of neural network to predict which character would come next.\n",
    "\n",
    "A recurrent neural network, however, is able to remember those characters because of its internal memory. It produces output, copies that output and loops it back into the network. \n",
    "\n",
    "Simply put: Recurrent neural networks add the immediate past to the present.\n",
    "\n",
    "Therefore, an RNN has two inputs: the present and the recent past. This is important because the sequence of data contains crucial information about what is coming next, which is why an RNN can do things other algorithms can’t.\n",
    "\n",
    "A feed-forward neural network assigns, like all other deep learning algorithms, a weight matrix to its inputs and then produces the output. Note that RNNs apply weights to the current and also to the previous input. Furthermore, a recurrent neural network will also tweak the weights for both gradient descent and backpropagation through time.\n",
    "\n",
    "# Types of Recurrent Neural Networks\n",
    "\n",
    "Types of Recurrent Neural Networks (RNNs)\n",
    "\n",
    "    One to One\n",
    "    One to Many\n",
    "    Many to One\n",
    "    Many to Many\n",
    "\n",
    "Also note that while feed-forward neural networks map one input to one output, RNNs can map one to many, many to many (translation) and many to one (classifying a voice).\n",
    "\n",
    "![](2024-03-01-22-10-18.png)\n",
    "\n",
    "# Recurrent Neural Networks and Backpropagation Through Time\n",
    "\n",
    "To understand the concept of backpropagation through time (BPTT), you’ll need to understand the concepts of forward and backpropagation first. We could spend an entire article discussing these concepts, so I will attempt to provide as simple a definition as possible.\n",
    "What Is Backpropagation?\n",
    "\n",
    "Backpropagation (BP or backprop) is known as a workhorse algorithm in machine learning. Backpropagation is used for calculating the gradient of an error function with respect to a neural network’s weights. The algorithm works its way backwards through the various layers of gradients to find the partial derivative of the errors with respect to the weights. Backprop then uses these weights to decrease error margins when training.\n",
    "\n",
    "In neural networks, you basically do forward-propagation to get the output of your model and check if this output is correct or incorrect, to get the error. Backpropagation is nothing but going backwards through your neural network to find the partial derivatives of the error with respect to the weights, which enables you to subtract this value from the weights.\n",
    "\n",
    "Those derivatives are then used by gradient descent, an algorithm that can iteratively minimize a given function. Then it adjusts the weights up or down, depending on which decreases the error. That is exactly how a neural network learns during the training process.\n",
    "\n",
    "So, with backpropagation you basically try to tweak the weights of your model while training.\n",
    "\n",
    "The image below illustrates the concept of forward propagation and backpropagation in a feed-forward neural network:\n",
    "\n",
    "![](2024-03-01-22-11-06.png)\n",
    "\n",
    "BPTT is basically just a fancy buzzword for doing backpropagation on an unrolled recurrent neural network. Unrolling is a visualization and conceptual tool, which helps you understand what’s going on within the network. Most of the time when implementing a recurrent neural network in the common programming frameworks, backpropagation is automatically taken care of, but you need to understand how it works to troubleshoot problems that may arise during the development process.\n",
    "\n",
    "You can view an RNN as a sequence of neural networks that you train one after another with backpropagation.\n",
    "\n",
    "The image below illustrates an unrolled RNN. On the left, the RNN is unrolled after the equal sign. Note there is no cycle after the equal sign since the different time steps are visualized and information is passed from one time step to the next. This illustration also shows why an RNN can be seen as a sequence of neural networks.\n",
    "unrolled version of RNN\n",
    "An unrolled version of RNN\n",
    "\n",
    "![](2024-03-01-22-12-01.png)\n",
    "\n",
    "If you do BPTT, the conceptualization of unrolling is required since the error of a given time step depends on the previous time step.\n",
    "\n",
    "Within BPTT the error is backpropagated from the last to the first time step, while unrolling all the time steps. This allows calculating the error for each time step, which allows updating the weights. Note that BPTT can be computationally expensive when you have a high number of time steps.\n",
    "\n",
    " \n",
    "# Common Problems of Recurrent Neural Networks\n",
    "\n",
    "While RNNs have been a difference-maker in the deep learning space, there are some issues to keep in mind: \n",
    "\n",
    "    Exploding gradients: This is when the algorithm, without much reason, assigns a stupidly high importance to the weights. Fortunately, this problem can be easily solved by truncating or squashing the gradients.\n",
    "     \n",
    "    Vanishing gradients: These occur when the values of a gradient are too small and the model stops learning or takes way too long as a result. Fortunately, it was solved through the concept of LSTM by Sepp Hochreiter and Juergen Schmidhuber.\n",
    "     \n",
    "    Complex training process: Because RNNs process data sequentially, this can result in a tedious training process.\n",
    "     \n",
    "    Difficulty with long sequences: The longer the sequence, the harder RNNs must work to remember past information.\n",
    "     \n",
    "    Inefficient methods: RNNs process data sequentially, which can be a slow and inefficient approach.  \n",
    "\n",
    " \n",
    "# Benefits of Recurrent Neural Networks \n",
    "\n",
    "The upsides of recurrent neural networks overshadow any challenges. Here are a few reasons RNNs have accelerated machine learning innovation:  \n",
    "\n",
    "    Extensive memory: RNNs can remember previous inputs and outputs, and this ability is enhanced with the help of LSTM networks.\n",
    "     \n",
    "    Greater accuracy: Because RNNs are able to learn from past experiences, they can make accurate predictions.\n",
    "     \n",
    "    Sequential data expertise: RNNs understand the temporal aspect of data, making them ideal for processing sequential data.\n",
    "     \n",
    "    Wide versatility: RNNs can handle sequential data like time series, audio and speech, giving them a broad range of applications.  \n",
    "\n",
    " \n",
    "# Recurrent Neural Networks and Long Short-Term Memory (LSTM) \n",
    "\n",
    "Long short-term memory networks (LSTMs) are an extension for RNNs, which basically extends the memory. Therefore, it is well suited to learn from important experiences that have very long time lags in between.\n",
    "What Is Long Short-Term Memory (LSTM)?\n",
    "\n",
    "Long short-term memory (LSTM) networks are an extension of RNN that extend the memory. LSTMs are used as the building blocks for the layers of a RNN. LSTMs assign data “weights” which helps RNNs to either let new information in, forget information or give it importance enough to impact the output.\n",
    "\n",
    "The units of an LSTM are used as building units for the layers of an RNN, often called an LSTM network.\n",
    "\n",
    "LSTMs enable RNNs to remember inputs over a long period of time. This is because LSTMs contain information in a memory, much like the memory of a computer. The LSTM can read, write and delete information from its memory.\n",
    "\n",
    "This memory can be seen as a gated cell, with gated meaning the cell decides whether or not to store or delete information (i.e., if it opens the gates or not), based on the importance it assigns to the information. The assigning of importance happens through weights, which are also learned by the algorithm. This simply means that it learns over time what information is important and what is not.\n",
    "\n",
    "In a long short-term memory cell you have three gates: input, forget and output gate. These gates determine whether or not to let new input in (input gate), delete the information because it isn’t important (forget gate), or let it impact the output at the current time step (output gate). Below is an illustration of an RNN with its three gates:\n",
    "\n",
    "![](2024-03-01-22-12-39.png)\n",
    "\n",
    "The gates in an LSTM are analog in the form of sigmoids, meaning they range from zero to one. The fact that they are analog enables them to do backpropagation.\n",
    "\n",
    "The problematic issue of vanishing gradients is solved through LSTM because it keeps the gradients steep enough, which keeps the training relatively short and the accuracy high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0f353-d7f9-4ef2-b4ab-98b1f0d2e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for the presence of GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if not gpus:\n",
    "    print(\"No GPU devices found.\")\n",
    "else:\n",
    "    print(\"Available GPU devices:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274a4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71208dc5-6b7f-40d3-9922-b3d2c28185f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334f0a8f-4291-4bff-98f9-d79fa61d87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "with open('C:\\\\Users\\\\manas\\\\Downloads\\\\sherlock-holm.es_stories_plain-text_advs.txt' , 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77345225-126d-442b-83c0-67c7e9db5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer process\n",
    "tokenizer = Tokenizer()\n",
    "#fit\n",
    "tokenizer.fit_on_texts([text])\n",
    "#assign length of word index\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc62491-9a3d-4c4a-b7de-0675489948cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'i': 3,\n",
       " 'to': 4,\n",
       " 'of': 5,\n",
       " 'a': 6,\n",
       " 'in': 7,\n",
       " 'that': 8,\n",
       " 'it': 9,\n",
       " 'he': 10,\n",
       " 'you': 11,\n",
       " 'was': 12,\n",
       " 'his': 13,\n",
       " 'is': 14,\n",
       " 'my': 15,\n",
       " 'have': 16,\n",
       " 'as': 17,\n",
       " 'with': 18,\n",
       " 'had': 19,\n",
       " 'which': 20,\n",
       " 'at': 21,\n",
       " 'for': 22,\n",
       " 'but': 23,\n",
       " 'me': 24,\n",
       " 'not': 25,\n",
       " 'be': 26,\n",
       " 'we': 27,\n",
       " 'from': 28,\n",
       " 'there': 29,\n",
       " 'this': 30,\n",
       " 'said': 31,\n",
       " 'upon': 32,\n",
       " 'so': 33,\n",
       " 'holmes': 34,\n",
       " 'him': 35,\n",
       " 'her': 36,\n",
       " 'she': 37,\n",
       " \"'\": 38,\n",
       " 'very': 39,\n",
       " 'your': 40,\n",
       " 'been': 41,\n",
       " 'all': 42,\n",
       " 'on': 43,\n",
       " 'no': 44,\n",
       " 'what': 45,\n",
       " 'one': 46,\n",
       " 'then': 47,\n",
       " 'were': 48,\n",
       " 'by': 49,\n",
       " 'are': 50,\n",
       " 'an': 51,\n",
       " 'would': 52,\n",
       " 'out': 53,\n",
       " 'when': 54,\n",
       " 'up': 55,\n",
       " 'man': 56,\n",
       " 'could': 57,\n",
       " 'has': 58,\n",
       " 'do': 59,\n",
       " 'into': 60,\n",
       " 'mr': 61,\n",
       " 'who': 62,\n",
       " 'little': 63,\n",
       " 'will': 64,\n",
       " 'if': 65,\n",
       " 'some': 66,\n",
       " 'now': 67,\n",
       " 'see': 68,\n",
       " 'down': 69,\n",
       " 'should': 70,\n",
       " 'our': 71,\n",
       " 'or': 72,\n",
       " 'they': 73,\n",
       " 'may': 74,\n",
       " 'well': 75,\n",
       " 'am': 76,\n",
       " 'us': 77,\n",
       " 'over': 78,\n",
       " 'more': 79,\n",
       " 'think': 80,\n",
       " 'room': 81,\n",
       " 'know': 82,\n",
       " 'shall': 83,\n",
       " 'about': 84,\n",
       " 'can': 85,\n",
       " 'before': 86,\n",
       " 'must': 87,\n",
       " 'only': 88,\n",
       " 'come': 89,\n",
       " 'than': 90,\n",
       " 'did': 91,\n",
       " 'time': 92,\n",
       " 'two': 93,\n",
       " 'other': 94,\n",
       " 'came': 95,\n",
       " 'them': 96,\n",
       " 'door': 97,\n",
       " 'back': 98,\n",
       " 'how': 99,\n",
       " 'good': 100,\n",
       " 'here': 101,\n",
       " 'any': 102,\n",
       " 'face': 103,\n",
       " 'might': 104,\n",
       " 'just': 105,\n",
       " 'matter': 106,\n",
       " 'house': 107,\n",
       " 'much': 108,\n",
       " 'hand': 109,\n",
       " 'case': 110,\n",
       " 'way': 111,\n",
       " 'where': 112,\n",
       " 'night': 113,\n",
       " 'yes': 114,\n",
       " 'heard': 115,\n",
       " 'such': 116,\n",
       " 'made': 117,\n",
       " 'nothing': 118,\n",
       " 'however': 119,\n",
       " 'found': 120,\n",
       " 'away': 121,\n",
       " 'day': 122,\n",
       " 'never': 123,\n",
       " 'quite': 124,\n",
       " 'morning': 125,\n",
       " 'own': 126,\n",
       " 'go': 127,\n",
       " 'after': 128,\n",
       " 'sherlock': 129,\n",
       " 'right': 130,\n",
       " 'their': 131,\n",
       " 'like': 132,\n",
       " 'tell': 133,\n",
       " 'last': 134,\n",
       " 'say': 135,\n",
       " 'left': 136,\n",
       " 'through': 137,\n",
       " 'saw': 138,\n",
       " 'most': 139,\n",
       " 'yet': 140,\n",
       " 'side': 141,\n",
       " 'asked': 142,\n",
       " 'eyes': 143,\n",
       " 'long': 144,\n",
       " 'took': 145,\n",
       " 'miss': 146,\n",
       " 'its': 147,\n",
       " 'once': 148,\n",
       " 'first': 149,\n",
       " 'street': 150,\n",
       " 'too': 151,\n",
       " 'every': 152,\n",
       " 'watson': 153,\n",
       " 'round': 154,\n",
       " 'young': 155,\n",
       " 'st': 156,\n",
       " 'still': 157,\n",
       " 'these': 158,\n",
       " 'find': 159,\n",
       " 'take': 160,\n",
       " 'small': 161,\n",
       " 'thought': 162,\n",
       " 'myself': 163,\n",
       " 'sir': 164,\n",
       " 'few': 165,\n",
       " 'light': 166,\n",
       " 'oh': 167,\n",
       " 'until': 168,\n",
       " 'off': 169,\n",
       " 'without': 170,\n",
       " 'himself': 171,\n",
       " 'hands': 172,\n",
       " 'make': 173,\n",
       " 'business': 174,\n",
       " 'father': 175,\n",
       " 'seen': 176,\n",
       " 'old': 177,\n",
       " 'window': 178,\n",
       " 'lady': 179,\n",
       " 'look': 180,\n",
       " 'three': 181,\n",
       " 'ever': 182,\n",
       " 'even': 183,\n",
       " 'friend': 184,\n",
       " 'let': 185,\n",
       " 'cried': 186,\n",
       " 'seemed': 187,\n",
       " 'again': 188,\n",
       " 'head': 189,\n",
       " 'went': 190,\n",
       " 'having': 191,\n",
       " 'put': 192,\n",
       " 'why': 193,\n",
       " 'done': 194,\n",
       " 'while': 195,\n",
       " 'those': 196,\n",
       " 'something': 197,\n",
       " 'doubt': 198,\n",
       " 'remarked': 199,\n",
       " 'open': 200,\n",
       " 'rather': 201,\n",
       " 'years': 202,\n",
       " 'though': 203,\n",
       " 'name': 204,\n",
       " 'indeed': 205,\n",
       " 'chair': 206,\n",
       " 'half': 207,\n",
       " 'perhaps': 208,\n",
       " 'get': 209,\n",
       " 'woman': 210,\n",
       " 'between': 211,\n",
       " 'give': 212,\n",
       " 'great': 213,\n",
       " 'course': 214,\n",
       " 'always': 215,\n",
       " 'mind': 216,\n",
       " 'enough': 217,\n",
       " 'knew': 218,\n",
       " 'end': 219,\n",
       " 'answered': 220,\n",
       " 'same': 221,\n",
       " 'far': 222,\n",
       " 'sat': 223,\n",
       " 'looking': 224,\n",
       " 'place': 225,\n",
       " 'table': 226,\n",
       " 'dear': 227,\n",
       " 'wife': 228,\n",
       " 'got': 229,\n",
       " 'police': 230,\n",
       " 'against': 231,\n",
       " 'also': 232,\n",
       " 'really': 233,\n",
       " 'red': 234,\n",
       " 'looked': 235,\n",
       " 'black': 236,\n",
       " 'better': 237,\n",
       " 'anything': 238,\n",
       " 'turned': 239,\n",
       " 'cannot': 240,\n",
       " 'behind': 241,\n",
       " 'told': 242,\n",
       " 'hardly': 243,\n",
       " 'hat': 244,\n",
       " 'front': 245,\n",
       " 'brought': 246,\n",
       " 'possible': 247,\n",
       " 'home': 248,\n",
       " 'understand': 249,\n",
       " 'life': 250,\n",
       " 'leave': 251,\n",
       " 'within': 252,\n",
       " 'work': 253,\n",
       " 'help': 254,\n",
       " 'thing': 255,\n",
       " 'both': 256,\n",
       " 'already': 257,\n",
       " 'suddenly': 258,\n",
       " 'strange': 259,\n",
       " 'gave': 260,\n",
       " 'words': 261,\n",
       " 'son': 262,\n",
       " 'whole': 263,\n",
       " 'fire': 264,\n",
       " 'point': 265,\n",
       " 'paper': 266,\n",
       " 'being': 267,\n",
       " 'papers': 268,\n",
       " 'minutes': 269,\n",
       " 'hair': 270,\n",
       " 'clear': 271,\n",
       " 'money': 272,\n",
       " 'wish': 273,\n",
       " 'gone': 274,\n",
       " 'under': 275,\n",
       " 'call': 276,\n",
       " 'sure': 277,\n",
       " 'whether': 278,\n",
       " \"'i\": 279,\n",
       " 'mrs': 280,\n",
       " 'set': 281,\n",
       " 'yourself': 282,\n",
       " 'certainly': 283,\n",
       " 'gentleman': 284,\n",
       " 'many': 285,\n",
       " 'pray': 286,\n",
       " 'another': 287,\n",
       " 'lay': 288,\n",
       " 'baker': 289,\n",
       " 'passed': 290,\n",
       " 'london': 291,\n",
       " 'large': 292,\n",
       " 'days': 293,\n",
       " 'five': 294,\n",
       " 'dark': 295,\n",
       " 'word': 296,\n",
       " 'met': 297,\n",
       " 'since': 298,\n",
       " 'whom': 299,\n",
       " 'does': 300,\n",
       " 'soon': 301,\n",
       " 'lord': 302,\n",
       " 'bed': 303,\n",
       " 'simon': 304,\n",
       " 'less': 305,\n",
       " 'during': 306,\n",
       " 'men': 307,\n",
       " 'four': 308,\n",
       " 'interest': 309,\n",
       " 'evening': 310,\n",
       " 'save': 311,\n",
       " 'across': 312,\n",
       " 'note': 313,\n",
       " 'lestrade': 314,\n",
       " 'returned': 315,\n",
       " 'read': 316,\n",
       " 'opened': 317,\n",
       " 'least': 318,\n",
       " 'strong': 319,\n",
       " 'among': 320,\n",
       " 'story': 321,\n",
       " 'stood': 322,\n",
       " 'country': 323,\n",
       " 'together': 324,\n",
       " 'believe': 325,\n",
       " 'facts': 326,\n",
       " 'doctor': 327,\n",
       " 'question': 328,\n",
       " 'ask': 329,\n",
       " 'moment': 330,\n",
       " 'none': 331,\n",
       " 'waiting': 332,\n",
       " 'fellow': 333,\n",
       " 'rucastle': 334,\n",
       " 'felt': 335,\n",
       " 'either': 336,\n",
       " \"o'clock\": 337,\n",
       " 'entered': 338,\n",
       " 'sitting': 339,\n",
       " 'use': 340,\n",
       " 'rushed': 341,\n",
       " 'mccarthy': 342,\n",
       " 'each': 343,\n",
       " 'crime': 344,\n",
       " 'singular': 345,\n",
       " 'part': 346,\n",
       " 'corner': 347,\n",
       " 'seven': 348,\n",
       " 'given': 349,\n",
       " 'hear': 350,\n",
       " 'hour': 351,\n",
       " 'else': 352,\n",
       " 'laid': 353,\n",
       " 'road': 354,\n",
       " 'going': 355,\n",
       " 'sound': 356,\n",
       " 'client': 357,\n",
       " 'best': 358,\n",
       " 'able': 359,\n",
       " 'became': 360,\n",
       " 'near': 361,\n",
       " 'instant': 362,\n",
       " 'hope': 363,\n",
       " 'forward': 364,\n",
       " 'used': 365,\n",
       " \"'you\": 366,\n",
       " 'stone': 367,\n",
       " 'family': 368,\n",
       " 'companion': 369,\n",
       " 'new': 370,\n",
       " 'heavy': 371,\n",
       " \"it's\": 372,\n",
       " 'turn': 373,\n",
       " 'several': 374,\n",
       " 'letter': 375,\n",
       " 'ran': 376,\n",
       " 'cut': 377,\n",
       " 'rooms': 378,\n",
       " 'threw': 379,\n",
       " 'true': 380,\n",
       " 'imagine': 381,\n",
       " 'six': 382,\n",
       " \"don't\": 383,\n",
       " 'past': 384,\n",
       " 'dr': 385,\n",
       " 'white': 386,\n",
       " 'known': 387,\n",
       " 'year': 388,\n",
       " 'floor': 389,\n",
       " 'hard': 390,\n",
       " 'ten': 391,\n",
       " 'obvious': 392,\n",
       " 'inspector': 393,\n",
       " 'coronet': 394,\n",
       " 'late': 395,\n",
       " 'manner': 396,\n",
       " 'ago': 397,\n",
       " 'death': 398,\n",
       " 'want': 399,\n",
       " 'feet': 400,\n",
       " 'coat': 401,\n",
       " 'taken': 402,\n",
       " 'station': 403,\n",
       " 'walked': 404,\n",
       " 'fear': 405,\n",
       " 'seems': 406,\n",
       " 'cry': 407,\n",
       " 'coming': 408,\n",
       " 'ah': 409,\n",
       " 'lamp': 410,\n",
       " 'spoke': 411,\n",
       " 'things': 412,\n",
       " 'marriage': 413,\n",
       " 'lost': 414,\n",
       " 'dress': 415,\n",
       " 'appeared': 416,\n",
       " 'present': 417,\n",
       " 'absolutely': 418,\n",
       " 'air': 419,\n",
       " 'above': 420,\n",
       " 'goose': 421,\n",
       " 'colonel': 422,\n",
       " 'blue': 423,\n",
       " 'visitor': 424,\n",
       " 'deep': 425,\n",
       " 'alone': 426,\n",
       " 'rose': 427,\n",
       " 'called': 428,\n",
       " 'letters': 429,\n",
       " 'cab': 430,\n",
       " 'drove': 431,\n",
       " 'god': 432,\n",
       " 'dressed': 433,\n",
       " 'remember': 434,\n",
       " 'mine': 435,\n",
       " 'reason': 436,\n",
       " 'keep': 437,\n",
       " 'held': 438,\n",
       " 'office': 439,\n",
       " 'sister': 440,\n",
       " 'week': 441,\n",
       " 'led': 442,\n",
       " 'bell': 443,\n",
       " 'steps': 444,\n",
       " 'address': 445,\n",
       " 'followed': 446,\n",
       " 'king': 447,\n",
       " 'photograph': 448,\n",
       " 'married': 449,\n",
       " 'second': 450,\n",
       " 'beside': 451,\n",
       " 'quick': 452,\n",
       " 'windows': 453,\n",
       " 'people': 454,\n",
       " 'began': 455,\n",
       " 'john': 456,\n",
       " 'chance': 457,\n",
       " 'easy': 458,\n",
       " 'heart': 459,\n",
       " 'fact': 460,\n",
       " 'square': 461,\n",
       " 'headed': 462,\n",
       " 'nature': 463,\n",
       " 'attention': 464,\n",
       " 'eye': 465,\n",
       " 'step': 466,\n",
       " 'outside': 467,\n",
       " 'cases': 468,\n",
       " 'likely': 469,\n",
       " 'twenty': 470,\n",
       " 'started': 471,\n",
       " 'anyone': 472,\n",
       " 'struck': 473,\n",
       " 'silence': 474,\n",
       " 'thank': 475,\n",
       " 'grey': 476,\n",
       " 'ready': 477,\n",
       " 'girl': 478,\n",
       " 'glancing': 479,\n",
       " 'passage': 480,\n",
       " 'sprang': 481,\n",
       " 'ground': 482,\n",
       " 'daughter': 483,\n",
       " 'bring': 484,\n",
       " 'next': 485,\n",
       " 'lane': 486,\n",
       " 'whose': 487,\n",
       " 'maid': 488,\n",
       " 'pocket': 489,\n",
       " 'standing': 490,\n",
       " 'sort': 491,\n",
       " 'poor': 492,\n",
       " 'town': 493,\n",
       " 'happened': 494,\n",
       " 'idea': 495,\n",
       " 'breakfast': 496,\n",
       " 'holder': 497,\n",
       " 'clair': 498,\n",
       " 'mystery': 499,\n",
       " 'cold': 500,\n",
       " 'position': 501,\n",
       " 'high': 502,\n",
       " 'clothes': 503,\n",
       " 'observed': 504,\n",
       " 'because': 505,\n",
       " 'carried': 506,\n",
       " 'person': 507,\n",
       " 'glanced': 508,\n",
       " 'hurried': 509,\n",
       " 'carriage': 510,\n",
       " 'drawn': 511,\n",
       " 'husband': 512,\n",
       " 'kind': 513,\n",
       " 'closed': 514,\n",
       " 'short': 515,\n",
       " 'bird': 516,\n",
       " 'hosmer': 517,\n",
       " 'occurred': 518,\n",
       " 'seeing': 519,\n",
       " 'mary': 520,\n",
       " 'order': 521,\n",
       " 'ha': 522,\n",
       " 'interesting': 523,\n",
       " 'doing': 524,\n",
       " 'important': 525,\n",
       " 'quiet': 526,\n",
       " 'need': 527,\n",
       " 'train': 528,\n",
       " 'afraid': 529,\n",
       " 'considerable': 530,\n",
       " 'body': 531,\n",
       " 'city': 532,\n",
       " 'sight': 533,\n",
       " 'points': 534,\n",
       " 'thin': 535,\n",
       " 'danger': 536,\n",
       " 'wedding': 537,\n",
       " 'public': 538,\n",
       " 'hunter': 539,\n",
       " 'account': 540,\n",
       " 'problem': 541,\n",
       " 'shown': 542,\n",
       " 'fashion': 543,\n",
       " 'matters': 544,\n",
       " 'written': 545,\n",
       " 'peculiar': 546,\n",
       " 'boy': 547,\n",
       " 'england': 548,\n",
       " 'character': 549,\n",
       " 'voice': 550,\n",
       " 'caught': 551,\n",
       " 'speak': 552,\n",
       " 'secret': 553,\n",
       " 'laughed': 554,\n",
       " 'close': 555,\n",
       " 'remarkable': 556,\n",
       " 'quietly': 557,\n",
       " 'towards': 558,\n",
       " 'mean': 559,\n",
       " 'drive': 560,\n",
       " 'cause': 561,\n",
       " 'turner': 562,\n",
       " 'hours': 563,\n",
       " 'fresh': 564,\n",
       " 'ourselves': 565,\n",
       " 'showed': 566,\n",
       " 'safe': 567,\n",
       " 'wilson': 568,\n",
       " 'advertisement': 569,\n",
       " 'earth': 570,\n",
       " 'experience': 571,\n",
       " 'everything': 572,\n",
       " 'opinion': 573,\n",
       " 'talk': 574,\n",
       " 'windibank': 575,\n",
       " 'mother': 576,\n",
       " 'adventure': 577,\n",
       " 'placed': 578,\n",
       " 'excellent': 579,\n",
       " 'extraordinary': 580,\n",
       " 'finally': 581,\n",
       " 'figure': 582,\n",
       " 'observe': 583,\n",
       " 'someone': 584,\n",
       " 'simple': 585,\n",
       " 'hall': 586,\n",
       " 'means': 587,\n",
       " 'sent': 588,\n",
       " 'hundred': 589,\n",
       " 'single': 590,\n",
       " 'slowly': 591,\n",
       " 'sign': 592,\n",
       " 'line': 593,\n",
       " 'lodge': 594,\n",
       " 'suppose': 595,\n",
       " 'child': 596,\n",
       " 'reached': 597,\n",
       " 'details': 598,\n",
       " 'wait': 599,\n",
       " 'others': 600,\n",
       " 'direction': 601,\n",
       " 'dead': 602,\n",
       " 'box': 603,\n",
       " 'return': 604,\n",
       " 'rest': 605,\n",
       " 'finger': 606,\n",
       " 'court': 607,\n",
       " 'effect': 608,\n",
       " 'colour': 609,\n",
       " 'advice': 610,\n",
       " 'pipe': 611,\n",
       " 'silent': 612,\n",
       " 'angel': 613,\n",
       " 'stepfather': 614,\n",
       " 'pool': 615,\n",
       " 'frank': 616,\n",
       " 'k': 617,\n",
       " 'neville': 618,\n",
       " 'stoner': 619,\n",
       " 'love': 620,\n",
       " 'remained': 621,\n",
       " 'glad': 622,\n",
       " 'itself': 623,\n",
       " 'almost': 624,\n",
       " 'laughing': 625,\n",
       " 'times': 626,\n",
       " 'serious': 627,\n",
       " 'gold': 628,\n",
       " 'news': 629,\n",
       " 'bedroom': 630,\n",
       " 'garden': 631,\n",
       " 'listened': 632,\n",
       " \"'and\": 633,\n",
       " 'nor': 634,\n",
       " 'afterwards': 635,\n",
       " 'entirely': 636,\n",
       " 'fell': 637,\n",
       " 'lips': 638,\n",
       " 'bright': 639,\n",
       " 'water': 640,\n",
       " 'low': 641,\n",
       " 'locked': 642,\n",
       " 'happy': 643,\n",
       " 'along': 644,\n",
       " \"i'll\": 645,\n",
       " 'james': 646,\n",
       " 'arthur': 647,\n",
       " 'league': 648,\n",
       " 'thumb': 649,\n",
       " 'dreadful': 650,\n",
       " 'appears': 651,\n",
       " 'glance': 652,\n",
       " 'sit': 653,\n",
       " 'boots': 654,\n",
       " 'pay': 655,\n",
       " 'morrow': 656,\n",
       " 'afternoon': 657,\n",
       " 'pulled': 658,\n",
       " \"won't\": 659,\n",
       " 'meet': 660,\n",
       " \"holmes'\": 661,\n",
       " 'drew': 662,\n",
       " 'making': 663,\n",
       " 'ring': 664,\n",
       " 'seem': 665,\n",
       " 'weeks': 666,\n",
       " 'foot': 667,\n",
       " 'wooden': 668,\n",
       " 'instantly': 669,\n",
       " 'surprised': 670,\n",
       " 'feel': 671,\n",
       " 'hatherley': 672,\n",
       " 'evidence': 673,\n",
       " 'miles': 674,\n",
       " 'innocent': 675,\n",
       " 'feeling': 676,\n",
       " 'geese': 677,\n",
       " 'boscombe': 678,\n",
       " 'irene': 679,\n",
       " 'adler': 680,\n",
       " 'machine': 681,\n",
       " 'lit': 682,\n",
       " 'twice': 683,\n",
       " 'chamber': 684,\n",
       " 'kindly': 685,\n",
       " 'double': 686,\n",
       " 'show': 687,\n",
       " 'explain': 688,\n",
       " 'yours': 689,\n",
       " 'importance': 690,\n",
       " 'examined': 691,\n",
       " 'brown': 692,\n",
       " 'comes': 693,\n",
       " 'sharp': 694,\n",
       " 'shoulders': 695,\n",
       " 'impression': 696,\n",
       " 'appearance': 697,\n",
       " 'broad': 698,\n",
       " 'trust': 699,\n",
       " 'confess': 700,\n",
       " 'surprise': 701,\n",
       " 'majesty': 702,\n",
       " 'mad': 703,\n",
       " 'lock': 704,\n",
       " 'arrived': 705,\n",
       " 'dropped': 706,\n",
       " 'whispered': 707,\n",
       " 'taking': 708,\n",
       " 'centre': 709,\n",
       " 'determined': 710,\n",
       " 'change': 711,\n",
       " 'key': 712,\n",
       " 'turning': 713,\n",
       " 'company': 714,\n",
       " 'events': 715,\n",
       " 'shook': 716,\n",
       " 'full': 717,\n",
       " 'third': 718,\n",
       " 'smiling': 719,\n",
       " 'yard': 720,\n",
       " 'dog': 721,\n",
       " 'sudden': 722,\n",
       " 'pale': 723,\n",
       " 'cleared': 724,\n",
       " 'dressing': 725,\n",
       " 'world': 726,\n",
       " 'armchair': 727,\n",
       " 'fancy': 728,\n",
       " 'walk': 729,\n",
       " 'caused': 730,\n",
       " 'german': 731,\n",
       " 'therefore': 732,\n",
       " 'excuse': 733,\n",
       " 'thirty': 734,\n",
       " 'certain': 735,\n",
       " 'view': 736,\n",
       " 'deal': 737,\n",
       " 'object': 738,\n",
       " 'probably': 739,\n",
       " 'witness': 740,\n",
       " 'whatever': 741,\n",
       " 'neither': 742,\n",
       " 'blood': 743,\n",
       " 'waited': 744,\n",
       " 'later': 745,\n",
       " 'engaged': 746,\n",
       " 'care': 747,\n",
       " 'slight': 748,\n",
       " 'cellar': 749,\n",
       " 'live': 750,\n",
       " 'says': 751,\n",
       " \"didn't\": 752,\n",
       " 'answer': 753,\n",
       " 'claim': 754,\n",
       " 'force': 755,\n",
       " 'yellow': 756,\n",
       " 'wrong': 757,\n",
       " 'coroner': 758,\n",
       " 'terrible': 759,\n",
       " 'truth': 760,\n",
       " 'openshaw': 761,\n",
       " 'toller': 762,\n",
       " 'band': 763,\n",
       " 'noble': 764,\n",
       " 'society': 765,\n",
       " 'pass': 766,\n",
       " 'swiftly': 767,\n",
       " 'top': 768,\n",
       " 'thick': 769,\n",
       " 'lying': 770,\n",
       " 'eight': 771,\n",
       " 'received': 772,\n",
       " 'wrote': 773,\n",
       " 'scene': 774,\n",
       " 'glass': 775,\n",
       " 'continued': 776,\n",
       " 'stairs': 777,\n",
       " 'bad': 778,\n",
       " 'pushed': 779,\n",
       " 'absolute': 780,\n",
       " 'promise': 781,\n",
       " 'difficult': 782,\n",
       " 'follow': 783,\n",
       " 'private': 784,\n",
       " 'result': 785,\n",
       " 'monday': 786,\n",
       " 'features': 787,\n",
       " 'wall': 788,\n",
       " 'evidently': 789,\n",
       " 'arms': 790,\n",
       " 'church': 791,\n",
       " 'shot': 792,\n",
       " \"'the\": 793,\n",
       " 'run': 794,\n",
       " 'smoke': 795,\n",
       " 'broke': 796,\n",
       " 'shoulder': 797,\n",
       " 'walking': 798,\n",
       " 'impossible': 799,\n",
       " 'age': 800,\n",
       " 'affair': 801,\n",
       " 'assistant': 802,\n",
       " 'common': 803,\n",
       " \"'oh\": 804,\n",
       " 'died': 805,\n",
       " 'clay': 806,\n",
       " 'sum': 807,\n",
       " 'bank': 808,\n",
       " 'amid': 809,\n",
       " 'early': 810,\n",
       " 'clearly': 811,\n",
       " 'nine': 812,\n",
       " 'presence': 813,\n",
       " 'darkness': 814,\n",
       " \"man's\": 815,\n",
       " 'reading': 816,\n",
       " 'uncle': 817,\n",
       " 'friends': 818,\n",
       " 'hotel': 819,\n",
       " 'professional': 820,\n",
       " 'frightened': 821,\n",
       " 'bent': 822,\n",
       " 'envelope': 823,\n",
       " 'den': 824,\n",
       " 'roylott': 825,\n",
       " 'ventilator': 826,\n",
       " 'scandal': 827,\n",
       " 'copper': 828,\n",
       " 'power': 829,\n",
       " 'master': 830,\n",
       " 'signs': 831,\n",
       " 'spoken': 832,\n",
       " 'deduce': 833,\n",
       " 'inside': 834,\n",
       " 'throwing': 835,\n",
       " 'example': 836,\n",
       " 'houses': 837,\n",
       " 'tried': 838,\n",
       " 'send': 839,\n",
       " 'wood': 840,\n",
       " 'situation': 841,\n",
       " 'expected': 842,\n",
       " 'faced': 843,\n",
       " 'running': 844,\n",
       " 'usual': 845,\n",
       " 'different': 846,\n",
       " 'nearly': 847,\n",
       " 'raise': 848,\n",
       " 'self': 849,\n",
       " 'besides': 850,\n",
       " \"he's\": 851,\n",
       " 'yesterday': 852,\n",
       " 'months': 853,\n",
       " 'slipped': 854,\n",
       " 'heavily': 855,\n",
       " 'ross': 856,\n",
       " 'building': 857,\n",
       " 'middle': 858,\n",
       " 'knowledge': 859,\n",
       " 'edge': 860,\n",
       " 'clue': 861,\n",
       " 'start': 862,\n",
       " 'missing': 863,\n",
       " 'trap': 864,\n",
       " 'moran': 865,\n",
       " 'charge': 866,\n",
       " 'break': 867,\n",
       " 'horrible': 868,\n",
       " 'horner': 869,\n",
       " 'gems': 870,\n",
       " 'bohemia': 871,\n",
       " 'pips': 872,\n",
       " 'lip': 873,\n",
       " 'beeches': 874,\n",
       " 'results': 875,\n",
       " 'complete': 876,\n",
       " 'keen': 877,\n",
       " 'deeply': 878,\n",
       " 'following': 879,\n",
       " 'official': 880,\n",
       " 'practice': 881,\n",
       " 'tall': 882,\n",
       " 'lived': 883,\n",
       " \"can't\": 884,\n",
       " 'often': 885,\n",
       " 'interested': 886,\n",
       " 'sheet': 887,\n",
       " 'writing': 888,\n",
       " 'precisely': 889,\n",
       " 'pair': 890,\n",
       " 'fifty': 891,\n",
       " \"there's\": 892,\n",
       " 'raised': 893,\n",
       " 'chin': 894,\n",
       " 'honour': 895,\n",
       " 'state': 896,\n",
       " 'passing': 897,\n",
       " 'purpose': 898,\n",
       " 'subject': 899,\n",
       " 'pretty': 900,\n",
       " 'inquiry': 901,\n",
       " 'investigation': 902,\n",
       " 'ill': 903,\n",
       " 'reach': 904,\n",
       " 'streets': 905,\n",
       " 'law': 906,\n",
       " 'action': 907,\n",
       " 'cigar': 908,\n",
       " 'number': 909,\n",
       " 'knows': 910,\n",
       " 'carry': 911,\n",
       " 'blow': 912,\n",
       " 'draw': 913,\n",
       " 'perfectly': 914,\n",
       " 'precious': 915,\n",
       " 'wonder': 916,\n",
       " 'broken': 917,\n",
       " 'wished': 918,\n",
       " 'possibly': 919,\n",
       " 'narrative': 920,\n",
       " 'real': 921,\n",
       " 'west': 922,\n",
       " 'statement': 923,\n",
       " 'human': 924,\n",
       " \"'no\": 925,\n",
       " 'piece': 926,\n",
       " 'beg': 927,\n",
       " 'lawn': 928,\n",
       " 'shining': 929,\n",
       " 'merryweather': 930,\n",
       " 'iron': 931,\n",
       " 'lens': 932,\n",
       " 'kept': 933,\n",
       " 'weary': 934,\n",
       " 'worn': 935,\n",
       " 'alive': 936,\n",
       " 'firm': 937,\n",
       " 'traces': 938,\n",
       " 'free': 939,\n",
       " 'looks': 940,\n",
       " 'trouble': 941,\n",
       " 'horror': 942,\n",
       " 'sometimes': 943,\n",
       " 'grew': 944,\n",
       " 'opium': 945,\n",
       " 'lascar': 946,\n",
       " 'madam': 947,\n",
       " 'bradstreet': 948,\n",
       " 'orange': 949,\n",
       " 'reasoning': 950,\n",
       " 'drawing': 951,\n",
       " 'throw': 952,\n",
       " 'beyond': 953,\n",
       " 'returning': 954,\n",
       " 'hot': 955,\n",
       " 'post': 956,\n",
       " 'quarter': 957,\n",
       " 'wanted': 958,\n",
       " 'rich': 959,\n",
       " 'seat': 960,\n",
       " 'influence': 961,\n",
       " 'acquaintance': 962,\n",
       " 'opening': 963,\n",
       " 'beautiful': 964,\n",
       " 'handed': 965,\n",
       " 'briony': 966,\n",
       " 'dozen': 967,\n",
       " 'neighbourhood': 968,\n",
       " 'wind': 969,\n",
       " 'search': 970,\n",
       " 'fine': 971,\n",
       " 'contrary': 972,\n",
       " \"woman's\": 973,\n",
       " 'fall': 974,\n",
       " 'creature': 975,\n",
       " 'arm': 976,\n",
       " 'questioning': 977,\n",
       " 'evil': 978,\n",
       " 'value': 979,\n",
       " 'remark': 980,\n",
       " 'thrust': 981,\n",
       " 'fortune': 982,\n",
       " 'learn': 983,\n",
       " 'presume': 984,\n",
       " 'couple': 985,\n",
       " 'ways': 986,\n",
       " 'sake': 987,\n",
       " 'huge': 988,\n",
       " \"'it\": 989,\n",
       " 'exceedingly': 990,\n",
       " \"'yes\": 991,\n",
       " 'lad': 992,\n",
       " 'fingers': 993,\n",
       " 'jones': 994,\n",
       " 'scotland': 995,\n",
       " 'narrow': 996,\n",
       " 'unless': 997,\n",
       " 'curious': 998,\n",
       " 'connection': 999,\n",
       " 'plain': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chek the tokens\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b36ae6-34d2-489c-8998-7306e1a55aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare ngrams\n",
    "input_sequences = []\n",
    "#split the sentence from '\\n'\n",
    "for line in text.split('\\n'):\n",
    "    #get tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f7e6b3-a5ff-48d2-a6f9-9e7a4ae27818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'adventures', 'of', 'sherlock', 'holmes']\n"
     ]
    }
   ],
   "source": [
    "setence_token = input_sequences[3] # [1, 1561, 5, 129, 34]\n",
    "sentence = []\n",
    "for token in setence_token:\n",
    "    sentence.append(list((tokenizer.word_index).keys())[list((tokenizer.word_index).values()).index(token)])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3a0b0a-81d7-4576-9563-9569081d08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum sentence length\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "# input sequences\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0163f32e-5ede-4f80-8640-ac76dc4bf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9745f8-0b42-403b-b8d2-03d8e3271b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert one-hot-encode\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f628e12-c383-4706-b753-de42a04e6f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,208,800\n",
      "Trainable params: 2,208,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424e9924-c8ed-4cb7-b6dd-487ce1953380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3010/3010 [==============================] - 21s 5ms/step - loss: 6.2292 - accuracy: 0.0781\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 5.4867 - accuracy: 0.1255\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 5.1067 - accuracy: 0.1480\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 4.7895 - accuracy: 0.1654\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 4.4944 - accuracy: 0.1830\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 4.2136 - accuracy: 0.2015\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 3.9461 - accuracy: 0.2264\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 17s 6ms/step - loss: 3.6925 - accuracy: 0.2558\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 3.4538 - accuracy: 0.2884\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 3.2278 - accuracy: 0.3230\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 17s 6ms/step - loss: 3.0210 - accuracy: 0.3563\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 2.8290 - accuracy: 0.3905\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 2.6519 - accuracy: 0.4229\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 2.4890 - accuracy: 0.4548\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 2.3383 - accuracy: 0.4827\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 2.2003 - accuracy: 0.5129\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 2.0708 - accuracy: 0.5389\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.9544 - accuracy: 0.5638\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 1.8482 - accuracy: 0.5860\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.7469 - accuracy: 0.6068\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.6576 - accuracy: 0.6253\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 1.5711 - accuracy: 0.6456\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 1.4957 - accuracy: 0.6613\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.4262 - accuracy: 0.6774\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.3593 - accuracy: 0.6922\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 1.3009 - accuracy: 0.7040\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 1.2456 - accuracy: 0.7154\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.1938 - accuracy: 0.7286\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.1453 - accuracy: 0.7389\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.1035 - accuracy: 0.7488\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.0620 - accuracy: 0.7586\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 1.0253 - accuracy: 0.7643\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.9917 - accuracy: 0.7726\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.9571 - accuracy: 0.7807\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.9289 - accuracy: 0.7862\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.9023 - accuracy: 0.7923\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 17s 6ms/step - loss: 0.8732 - accuracy: 0.7998\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.8542 - accuracy: 0.8019\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.8306 - accuracy: 0.8081\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.8101 - accuracy: 0.8121\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.7924 - accuracy: 0.8148\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.7702 - accuracy: 0.8198\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 0.7561 - accuracy: 0.8233\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.7399 - accuracy: 0.8267\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.7262 - accuracy: 0.8291\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.7123 - accuracy: 0.8330\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6959 - accuracy: 0.8356\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6872 - accuracy: 0.8371\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 17s 6ms/step - loss: 0.6729 - accuracy: 0.8415\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6629 - accuracy: 0.8431\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6539 - accuracy: 0.8446\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6498 - accuracy: 0.8441\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6352 - accuracy: 0.8486\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 16s 5ms/step - loss: 0.6297 - accuracy: 0.8490\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 0.6235 - accuracy: 0.8492\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 0.6170 - accuracy: 0.8516\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 0.6074 - accuracy: 0.8524\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 0.6045 - accuracy: 0.8533\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 15s 5ms/step - loss: 0.5985 - accuracy: 0.8549\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 19s 6ms/step - loss: 0.5906 - accuracy: 0.8562\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5869 - accuracy: 0.8570\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5836 - accuracy: 0.8566\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5803 - accuracy: 0.8568\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5710 - accuracy: 0.8595\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5700 - accuracy: 0.8593\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5667 - accuracy: 0.8593\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5580 - accuracy: 0.8617\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5609 - accuracy: 0.8601\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5536 - accuracy: 0.8615\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5530 - accuracy: 0.8621\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5521 - accuracy: 0.8611\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5474 - accuracy: 0.8623\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5384 - accuracy: 0.8657\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5435 - accuracy: 0.8634\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5406 - accuracy: 0.8631\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5337 - accuracy: 0.8645\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5348 - accuracy: 0.8649\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5342 - accuracy: 0.8637\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5296 - accuracy: 0.8661\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5312 - accuracy: 0.8645\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5210 - accuracy: 0.8680\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5289 - accuracy: 0.8643\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5219 - accuracy: 0.8670\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5202 - accuracy: 0.8664\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5235 - accuracy: 0.8646\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5199 - accuracy: 0.8656\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5184 - accuracy: 0.8670\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5197 - accuracy: 0.8650\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5162 - accuracy: 0.8671\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5121 - accuracy: 0.8671\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5140 - accuracy: 0.8663\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5109 - accuracy: 0.8678\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5152 - accuracy: 0.8658\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5144 - accuracy: 0.8656\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5057 - accuracy: 0.8683\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5116 - accuracy: 0.8670\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5092 - accuracy: 0.8674\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5062 - accuracy: 0.8677\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 25s 8ms/step - loss: 0.5086 - accuracy: 0.8668\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 24s 8ms/step - loss: 0.5122 - accuracy: 0.8659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1ff5cfa00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#fit the model\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6e92ad8-1f3b-4224-966c-a8df819f2e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 466ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "I will close the door if we were out to a man if\n"
     ]
    }
   ],
   "source": [
    "# determine a text\n",
    "seed_text = \"I will close the door if\"\n",
    "# predict word number\n",
    "next_words = 7\n",
    "\n",
    "for _ in range(next_words):\n",
    "    # convert to token\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    # pad sequences\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    # model prediction\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    # get predicted word\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted[0]:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
